# Serve vLLM on Trillium TPUs (v6e):

This repository provides examples demonstrating how to deploy and serve vLLM on Trillium TPUs using GCE (Google Compute Engine) for a select set of models.

- [Llama3-8b](./Llama3-8b/README.md)
- [Qwen2.5-32B](./Qwen2.5-32B/README.md)
- [Llama-3.3-70B](./Llama3.3-70b/README.md)

These models were chosen for demonstration purposes only. You can serve any model from this list: [vLLM Supported Models](https://docs.vllm.ai/en/latest/models/supported_models.html)

If you are looking for GKE-based deployment, please refer to this documentation: [Serve an LLM using TPU Trillium on GKE with vLLM](https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-vllm-tpu)

