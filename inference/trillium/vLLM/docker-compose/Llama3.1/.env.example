# This file provides the default configuration for running Llama3.1 models.
# Copy this file to .env and edit it with your specific values.

# Docker image to use
DOCKER_URI=vllm/vllm-tpu:nightly

# Your Hugging Face token for downloading models
HF_TOKEN=<your-hugging-face-token-here>

# Name of the user to tag the container (optional, defaults to current user)
# USER=your-name

# --- Server Configuration ---
# Seed for reproducibility
SEED=42
# GPU memory utilization
GPU_MEMORY_UTILIZATION=0.98
# Add '--disable-log-requests' to disable request logging, or leave blank to enable.
DISABLE_LOG_REQUESTS=--disable-log-requests

# --- Llama3.1-70B Configuration (Default) ---
SHM_SIZE=150gb
MODEL_NAME=meta-llama/Llama-3.1-70B-Instruct
MAX_MODEL_LEN=4096
TP=8
MAX_NUM_BATCHED_TOKENS=2048
MAX_NUM_SEQS=256

# --- Llama3.1-8B Configuration ---
# Uncomment the lines below to switch to the 8B model.
#
# SHM_SIZE=17gb
# MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct
# MAX_MODEL_LEN=4096
# TP=1
# MAX_NUM_BATCHED_TOKENS=1024
# MAX_NUM_SEQS=128
