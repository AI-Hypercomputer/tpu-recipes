services:
  vllm:
    image: ${DOCKER_URI:-vllm/vllm-tpu:nightly}
    privileged: true
    network_mode: "host"
    volumes:
      - /dev/shm:/dev/shm
    shm_size: ${SHM_SIZE}
    ports:
      - "8000:8000"
    environment:
      - HF_HOME=/dev/shm
      - HF_TOKEN=${HF_TOKEN}
      - SEED=${SEED}
      - MODEL_NAME=${MODEL_NAME}
      # Pass multi-modal vars to the container for the benchmark command
      - LIMIT_MM_PER_PROMPT=${LIMIT_MM_PER_PROMPT}
      - MM_PROCESSOR_KWARGS=${MM_PROCESSOR_KWARGS}
      - GUIDED_DECODING_BACKEND=${GUIDED_DECODING_BACKEND}
      - DISABLE_CHUNKED_MM_INPUT=${DISABLE_CHUNKED_MM_INPUT}
    command: >
      vllm serve ${MODEL_NAME}
      --seed ${SEED}
      ${DISABLE_LOG_REQUESTS}
      --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION}
      --tensor-parallel-size ${TP}
      --max-model-len ${MAX_MODEL_LEN}
      --limit-mm-per-prompt ${LIMIT_MM_PER_PROMPT}
      --mm-processor-kwargs ${MM_PROCESSOR_KWARGS}
      --guided-decoding-backend ${GUIDED_DECODING_BACKEND}
      ${DISABLE_CHUNKED_MM_INPUT}
