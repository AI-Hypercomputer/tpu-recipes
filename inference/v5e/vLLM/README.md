# Serve vLLM on v5e TPUs:

This repository provides examples demonstrating how to deploy and serve vLLM on v5e TPUs using GCE (Google Compute Engine) for a select set of models.

- [Llama3-8b](./Llama3-8b/README.md)
- [Qwen2.5-32B](./Qwen2.5-32B/README.md)

These models were chosen for demonstration purposes only. You can serve any model from this list: [vLLM Supported Models](https://docs.vllm.ai/en/latest/models/supported_models.html)

If you are looking for GKE-based deployment, please refer to this documentation: [Serve an LLM using TPU Trillium on GKE with vLLM](https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-vllm-tpu)

To serve vLLM on v6e TPUs, please refer to this page: [Serve vLLM on Trillium TPUs (v6e)](../../trillium/vLLM/README.md)

